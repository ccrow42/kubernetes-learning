 ## First, we are going to learn about snapshots and clones.

 # Install the snapshot beta CRDs and Controller
 # install CRD with Beta release
kubectl create -f  https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl create -f  https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl create -f  https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml

#Add the snap controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml


# The above extends the kubernetes API so that snapshots are supported. It also adds a controller to manage them

# New lets install the purestorage volumeSnapshotclass:
kubectl apply -f https://raw.githubusercontent.com/purestorage/pso-csi/master/pure-pso/snapshotclass.yaml

# You will need a snapshot class for every CSI driver you have (portworx being another example)


Now let's head over to the minio-snap directory and run through the steps:

kubectl create namespace lesson05-1
kubectl -n lesson05-1 apply -f 1_*
kubectl -n lesson05-1 describe PersistentVolumeClaim

kubectl -n lesson05-1 apply -f 2_*
kubectl -n lesson05-1 describe pods

kubectl -n lesson05-1 apply -f 3_*
kubectl -n lesson05-1 describe services

# Connect to the service and put some data on minio

kubectl -n lesson05-1 apply -f 4_*
kubectl -n lesson05-1 describe volumesnapshots

# (Optional) Delete some data from minio

kubectl -n lesson05-1 apply -f 5_*
kubectl -n lesson05-1 describe PersistentVolumeClaim

kubectl -n lesson05-1 apply -f 6_*
kubectl -n lesson05-1 apply -f 7_*
kubectl -n lesson05-1 get service

# Connect to the new service and look at what data is there

# The above is dense. Pause at every step to understand what is happening behind the scenes

# By the way, the above example was the demo I gave during my vTeams interview

## MySQL stateful set

# Change directory to the mysql-statefulset directory

kubectl create namespace lesson05-2

kubectl -n lesson05-2 apply -f mysql-configmap.yaml

kubectl -n lesson05-2 apply -f mysql-services.yaml

kubectl -n lesson05-2 apply -f mysql-statefulset.yaml

# Watch the app come up:
kubectl -n lesson05-2 get pods -l app=mysql --watch

# Push some data to the master:
kubectl -n lesson05-2 run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\
  mysql -h mysql-0.mysql <<EOF
CREATE DATABASE test;
CREATE TABLE test.messages (message VARCHAR(250));
INSERT INTO test.messages VALUES ('hello');
EOF

# Test that the data is there:
kubectl -n lesson05-2 run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
   mysql -h mysql-read -e "SELECT * FROM test.messages"

# Loop through the mysql servers. It is helpful to have this running in a different window
kubectl -n lesson05-2 run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
   bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"

# The above command is a little crazy (and my favorite command we have run so far, it shows a short lived container). It is creating a new mssql connection every second and reading the server_id
# How is it cycling through the different pods? Services match their selector and round robin automatically
# what is the difference between the two services?

# Simulate downtime by moving the mysql binary out of the way
kubectl -n lesson05-2 exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off

kubectl -n lesson05-2 get pod mysql-2

kubectl -n lesson05-2 exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql

#Questions:
# How did the system know that the pod was down? Talk about Liveness probes and Readiness probes

# Delete a pod
kubectl -n lesson05-2 delete pod mysql-2

# Scale the number of slaves
kubectl -n lesson05-2 scale statefulset mysql  --replicas=5

kubectl -n lesson05-2 get pods -l app=mysql --watch

kubectl -n lesson05-2 scale statefulset mysql  --replicas=3



## Backups!!!

# go to the velero directory

## Pause Here!!! You need to modify the bucket name!!!
# Install velero:
./velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.0.0 \
    --bucket <ccrow[01-14]> \
    --secret-file ./velero-creds \
    --use-volume-snapshots=false \
    --use-restic=true \
    --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://10.228.112.79

# Start the backup
# Keep in mind that the below grabs everything! Velero uses selectors just like everything else. Try the namespace selector!
./velero backup create ccrow01-01 --default-volumes-to-restic

# watch the backup:
watch ./velero backup describe ccrow01-01

# get a list of backups
./velero backup get


# Now delete a namespace!!!

kubectl delete namespace lesson05-1

# test that minio is down
# We may have to do some intense troubleshooting here if the deletion doesn't work

./velero restore create --from-backup ccrow01-01 --include-namespaces lesson05-1

# now, some things will not restore, check to see what worked. Why did the second pvc now get restored?
# Second, we are getting close to the edge of what a single node kubernetes environment can do, so you may have resource constraints
# Troubleshoot with describe commands
